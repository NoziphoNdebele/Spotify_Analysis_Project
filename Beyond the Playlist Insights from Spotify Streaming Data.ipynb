{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Title: Beyond the Playlist: Insights from Spotify Streaming Data\n",
    "#### Done By: Nozipho Sithembiso Ndebele & Thabisisle Xaba\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"alexander-shatov-JlO3-oY5ZlQ-unsplash-scaled.jpg\" alt=\"Anime Image\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#BC> Background Context</a>\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Data Collection and Description</a>\n",
    "\n",
    "<a href=#three>3. Loading Data </a>\n",
    "\n",
    "<a href=#four>4. Data Cleaning and Filtering</a>\n",
    "\n",
    "<a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#six>6. Modeling </a>\n",
    "\n",
    "<a href=#seven>7. Evaluation and Validation</a>\n",
    "\n",
    "<a href=#eight>8. Final Model</a>\n",
    "\n",
    "<a href=#nine>9. Conclusion and Future Work</a>\n",
    "\n",
    "<a href=#ten>10. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " <a id=\"BC\"></a>\n",
    "## **Background Context**\n",
    "\n",
    "### Purpose\n",
    "This project aims to analyze Spotify user data to gain insights into music streaming behaviors, trends, and preferences. By leveraging data visualization techniques, the project will explore listening patterns, track popularity, and user engagement with different artists and albums over time.\n",
    "\n",
    "### Significance\n",
    "Understanding music streaming behavior is valuable for multiple applications, such as:\n",
    "\n",
    "* Identifying the most played tracks, artists, and albums over a period of time\n",
    "\n",
    "* Analyzing the impact of time, mood, and external factors on music choices\n",
    "\n",
    "* Helping artists, record labels, and streaming platforms improve recommendations and user experiences\n",
    "\n",
    "* Understanding the reasons behind track play and end events to optimize playlist curation\n",
    "\n",
    "By applying data visualization techniques to this dataset, we aim to uncover patterns in user listening habits and provide actionable insights for both music industry professionals and everyday listeners.\n",
    "\n",
    "### Problem Domain\n",
    "Spotify generates vast amounts of user data, including track plays, timestamps, and listening durations. Effectively interpreting this data requires the use of visualization techniques to extract meaningful insights and trends.\n",
    "### Challenges\n",
    "* Data Volume: Handling and processing large amounts of streaming data efficiently.\n",
    "\n",
    "* User Behavior Complexity: Understanding listening patterns influenced by factors like time of day, mood, and device type.\n",
    "\n",
    "* Feature Engineering: Identifying key attributes that contribute to user engagement and preferences.\n",
    "\n",
    "### Key Questions\n",
    "* What are the most popular songs, artists, and albums in the dataset?\n",
    "\n",
    "* How do listening habits change over time (daily, weekly, monthly trends)?\n",
    "\n",
    "* What factors influence track play and end reasons?\n",
    "\n",
    "* Can we identify user-specific music preferences based on historical data?\n",
    "\n",
    "* How do external factors (e.g., time of day, weekday vs. weekend) affect listening behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#one></a>\n",
    "## **Importing Packages**\n",
    "\n",
    "### Purpose\n",
    "To set up the Python environment with the necessary libraries for data manipulation, visualization, and machine learning. These libraries will facilitate data preprocessing, feature extraction, model training, and evaluation.\n",
    "\n",
    "### Details\n",
    "* Pandas: For handling and analyzing data.\n",
    "\n",
    "* NumPy: For numerical operations.\n",
    "\n",
    "* Matplotlib/Seaborn: For data visualization to understand trends and patterns.\n",
    "\n",
    "* scikit-learn: For building and evaluating machine learning models.\n",
    "\n",
    "* NLTK/Spacy: For text preprocessing and natural language processing tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nozih\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages  \n",
    "\n",
    "# Data manipulation and analysis  \n",
    "import pandas as pd  # Pandas for data handling  \n",
    "import numpy as np  # NumPy for numerical operations  \n",
    "\n",
    "# Data visualization  \n",
    "import matplotlib.pyplot as plt  # Matplotlib for static plots  \n",
    "import seaborn as sns  # Seaborn for statistical visualization  \n",
    "import plotly.express as px  # Plotly for interactive plots  \n",
    "\n",
    "# Natural Language Processing  \n",
    "import nltk  # Natural Language Toolkit  \n",
    "from nltk.corpus import stopwords  # Stopword removal  \n",
    "from nltk.tokenize import word_tokenize  # Tokenization  \n",
    "import re  # Regular expressions for text cleaning  \n",
    "\n",
    "# Configure visualization settings\n",
    "sns.set(style='whitegrid')  # Set the default style for Seaborn plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Set default figure size for Matplotlib\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings  # Import the warnings module\n",
    "warnings.filterwarnings('ignore')  # Ignore all warning messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#two></a>\n",
    "## **Data Collection and Description**\n",
    "### Purpose\n",
    "This section describes how the Spotify dataset was collected and provides insights into its structure. Understanding the dataset's composition is crucial for effective data visualization and interpretation.\n",
    "\n",
    "### Details\n",
    "The dataset consists of complete music streaming history data from Spotify users, including track details, timestamps, and listening behaviors.\n",
    "\n",
    "* Source: The dataset is sourced from Spotify's user data exports, APIs, or publicly available repositories like Kaggle.\n",
    "\n",
    "* Method of Collection: Data was gathered by tracking user interactions with Spotify, recording timestamps, track metadata, and reasons for playing or stopping each song.\n",
    "\n",
    "* Size: The dataset includes thousands of records capturing user listening behavior.\n",
    "\n",
    "* Scope: Covers various aspects of user engagement, including song play duration, artist preferences, and playlist interactions.\n",
    "\n",
    "* Types of Data:\n",
    "\n",
    "  * spotify_track_uri – Unique identifier for each track\n",
    "\n",
    "  * track_name – Name of the song played\n",
    "\n",
    "  * artist_name – Name of the artist performing the track\n",
    "\n",
    "  * album_name – Name of the album the track belongs to\n",
    "\n",
    "  * played_at – Timestamp when the track was played\n",
    "\n",
    "  * reason_start – Reason for playing the track (e.g., autoplay, user selection)\n",
    "\n",
    "  * reason_end – Reason for stopping the track (e.g., track end, user skip)\n",
    "\n",
    "By leveraging this dataset, we aim to create meaningful visualizations that provide deep insights into user streaming habits and music consumption trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#three></a>\n",
    "## **Loading Data**\n",
    "### Purpose\n",
    "The purpose of this section is to load the dataset into the notebook for further manipulation and analysis. This is the first step in working with the data, and it allows us to inspect the raw data and get a sense of its structure.\n",
    "\n",
    "### Details\n",
    "In this section, we will load the dataset into a Pandas DataFrame and display the first few rows to understand what the raw data looks like. This will help in planning the next steps of data cleaning and analysis.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a Pandas DataFrame\n",
    "\n",
    "# The dataset is stored in a CSV file named 'Domestic violence.csv'\n",
    "df = pd.read_csv('spotify_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is the original dataset (DataFrame), this creates a copy of it\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Now 'df_copy' is an independent copy of 'df'. Changes to 'df_copy' won't affect 'df'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_track_uri</th>\n",
       "      <th>ts</th>\n",
       "      <th>platform</th>\n",
       "      <th>ms_played</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>album_name</th>\n",
       "      <th>reason_start</th>\n",
       "      <th>reason_end</th>\n",
       "      <th>shuffle</th>\n",
       "      <th>skipped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2J3n32GeLmMjwuAzyhcSNe</td>\n",
       "      <td>2013-07-08 02:44:34</td>\n",
       "      <td>web player</td>\n",
       "      <td>3185</td>\n",
       "      <td>Say It, Just Say It</td>\n",
       "      <td>The Mowgli's</td>\n",
       "      <td>Waiting For The Dawn</td>\n",
       "      <td>autoplay</td>\n",
       "      <td>clickrow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1oHxIPqJyvAYHy0PVrDU98</td>\n",
       "      <td>2013-07-08 02:45:37</td>\n",
       "      <td>web player</td>\n",
       "      <td>61865</td>\n",
       "      <td>Drinking from the Bottle (feat. Tinie Tempah)</td>\n",
       "      <td>Calvin Harris</td>\n",
       "      <td>18 Months</td>\n",
       "      <td>clickrow</td>\n",
       "      <td>clickrow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487OPlneJNni3NWC8SYqhW</td>\n",
       "      <td>2013-07-08 02:50:24</td>\n",
       "      <td>web player</td>\n",
       "      <td>285386</td>\n",
       "      <td>Born To Die</td>\n",
       "      <td>Lana Del Rey</td>\n",
       "      <td>Born To Die - The Paradise Edition</td>\n",
       "      <td>clickrow</td>\n",
       "      <td>unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5IyblF777jLZj1vGHG2UD3</td>\n",
       "      <td>2013-07-08 02:52:40</td>\n",
       "      <td>web player</td>\n",
       "      <td>134022</td>\n",
       "      <td>Off To The Races</td>\n",
       "      <td>Lana Del Rey</td>\n",
       "      <td>Born To Die - The Paradise Edition</td>\n",
       "      <td>trackdone</td>\n",
       "      <td>clickrow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GgAAB0ZMllFhbNc3mAodO</td>\n",
       "      <td>2013-07-08 03:17:52</td>\n",
       "      <td>web player</td>\n",
       "      <td>0</td>\n",
       "      <td>Half Mast</td>\n",
       "      <td>Empire Of The Sun</td>\n",
       "      <td>Walking On A Dream</td>\n",
       "      <td>clickrow</td>\n",
       "      <td>nextbtn</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        spotify_track_uri                   ts    platform  ms_played  \\\n",
       "0  2J3n32GeLmMjwuAzyhcSNe  2013-07-08 02:44:34  web player       3185   \n",
       "1  1oHxIPqJyvAYHy0PVrDU98  2013-07-08 02:45:37  web player      61865   \n",
       "2  487OPlneJNni3NWC8SYqhW  2013-07-08 02:50:24  web player     285386   \n",
       "3  5IyblF777jLZj1vGHG2UD3  2013-07-08 02:52:40  web player     134022   \n",
       "4  0GgAAB0ZMllFhbNc3mAodO  2013-07-08 03:17:52  web player          0   \n",
       "\n",
       "                                      track_name        artist_name  \\\n",
       "0                            Say It, Just Say It       The Mowgli's   \n",
       "1  Drinking from the Bottle (feat. Tinie Tempah)      Calvin Harris   \n",
       "2                                    Born To Die       Lana Del Rey   \n",
       "3                               Off To The Races       Lana Del Rey   \n",
       "4                                      Half Mast  Empire Of The Sun   \n",
       "\n",
       "                           album_name reason_start reason_end  shuffle  \\\n",
       "0                Waiting For The Dawn     autoplay   clickrow    False   \n",
       "1                           18 Months     clickrow   clickrow    False   \n",
       "2  Born To Die - The Paradise Edition     clickrow    unknown    False   \n",
       "3  Born To Die - The Paradise Edition    trackdone   clickrow    False   \n",
       "4                  Walking On A Dream     clickrow    nextbtn    False   \n",
       "\n",
       "   skipped  \n",
       "0    False  \n",
       "1    False  \n",
       "2    False  \n",
       "3    False  \n",
       "4    False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset to get a sense of what the raw data looks like\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149860, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of rows and columns in the dataset to understand its size\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 149860 entries, 0 to 149859\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   spotify_track_uri  149860 non-null  object\n",
      " 1   ts                 149860 non-null  object\n",
      " 2   platform           149860 non-null  object\n",
      " 3   ms_played          149860 non-null  int64 \n",
      " 4   track_name         149860 non-null  object\n",
      " 5   artist_name        149860 non-null  object\n",
      " 6   album_name         149860 non-null  object\n",
      " 7   reason_start       149717 non-null  object\n",
      " 8   reason_end         149743 non-null  object\n",
      " 9   shuffle            149860 non-null  bool  \n",
      " 10  skipped            149860 non-null  bool  \n",
      "dtypes: bool(2), int64(1), object(8)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of the dataset\n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#four></a>\n",
    "## **Data Cleaning and Filtering**\n",
    "Before analyzing the data, it is crucial to clean and filter it. This process involves handling missing values, removing outliers, correcting errors, and possibly reducing the data by filtering out irrelevant features. These steps ensure that the analysis is based on accurate and reliable data.\n",
    "\n",
    "Details\n",
    "In this section, we will:\n",
    "\n",
    "* Check for Missing Values: Identify if there are any missing values in the dataset and handle them accordingly.\n",
    "* Remove Duplicates: Ensure there are no duplicate rows that could skew the analysis.\n",
    "* Correct Errors: Look for and correct any obvious data entry errors.\n",
    "* Filter Data: Depending on the analysis requirements, filter the data to include only relevant records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for missing values in the dataset\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check for missing values in the dataset and display the number of missing values per column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataset to check for missing values.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: A series showing the number of missing values for each column.\n",
    "    \"\"\"\n",
    "     # Check for missing values in the dataset and display them\n",
    "    print(\"Missing values per column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values)\n",
    "    return missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "spotify_track_uri      0\n",
      "ts                     0\n",
      "platform               0\n",
      "ms_played              0\n",
      "track_name             0\n",
      "artist_name            0\n",
      "album_name             0\n",
      "reason_start         143\n",
      "reason_end           117\n",
      "shuffle                0\n",
      "skipped                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "missing_values = check_missing_values(df_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dataset, missing values were identified in the reason_start (143) and reason_end (117) columns. All other columns are complete, ensuring data integrity for most features. Depending on the analysis goals, handling these missing values may be necessary through imputation or other data-cleaning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Imputes missing values in the 'reason_start' and 'reason_end' columns with 'Unknown'.\n",
    "    \n",
    "    Args:\n",
    "    df (pandas.DataFrame): The dataframe to process.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Check initial missing values\n",
    "    missing_before = df[['reason_start', 'reason_end']].isnull().sum().sum()\n",
    "    print(f\"\\nMissing values before imputation: {missing_before}\")\n",
    "\n",
    "    # Impute missing values\n",
    "    df[['reason_start', 'reason_end']] = df[['reason_start', 'reason_end']].fillna(\"Unknown\")\n",
    "\n",
    "    # Check missing values after imputation\n",
    "    missing_after = df[['reason_start', 'reason_end']].isnull().sum().sum()\n",
    "    print(f\"Missing values after imputation: {missing_after}\")\n",
    "\n",
    "    if missing_after == 0:\n",
    "        print(\"All missing values have been successfully replaced with 'Unknown'.\")\n",
    "    else:\n",
    "        print(\"Some missing values remain. Please review the data.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before imputation: 260\n",
      "Missing values after imputation: 0\n",
      "All missing values have been successfully replaced with 'Unknown'.\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "df_copy = impute_missing_values(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in the dataset and removes them if any are found.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The dataframe to check for duplicate rows.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with duplicate rows removed, if any existed.\n",
    "    \"\"\"\n",
    "    # Check for duplicate rows\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
    "    \n",
    "    # Remove duplicates if any exist\n",
    "    if duplicate_rows > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\"Duplicate rows removed. Updated dataframe has {len(df)} rows.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 1185\n",
      "Duplicate rows removed. Updated dataframe has 148675 rows.\n"
     ]
    }
   ],
   "source": [
    "df_copy = remove_duplicates(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon reviewing the dataset, 1185 duplicate rows were found and removed. This ensures that all records are unique, and no further action is required for data deduplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving the Cleaned Dataset**\n",
    "### Purpose\n",
    "\n",
    "This section outlines how to save the cleaned dataset for future use. Saving the dataset ensures that the data cleaning process does not need to be repeated and allows for consistent use in subsequent analyses.\n",
    "\n",
    "### Details\n",
    "\n",
    "We will save the cleaned dataset as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Save the cleaned dataset to a new CSV file\n",
    "\n",
    "def save_cleaned_dataset(df, filename='cleaned_spotify_history.csv'):\n",
    "    \"\"\"\n",
    "    Saves the cleaned dataframe to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The cleaned dataframe to save.\n",
    "    filename (str): The name of the file to save the dataframe to (default is 'cleaned_domestic_violence.csv').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Save the cleaned dataset to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Cleaned dataset saved successfully as '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully as 'cleaned_spotify_history.csv'.\n"
     ]
    }
   ],
   "source": [
    "save_cleaned_dataset(df_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#five></a>\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "It is the process of analyzing datasets to summarize key features, often through visualization methods. It aims to discover patterns, spot anomalies, and formulate hypotheses for deeper insights, which informs subsequent analysis.\n",
    "#### Advantages\n",
    "\n",
    "- Helps in understanding the data before modeling.\n",
    "- Provides insights that guide feature selection and engineering.\n",
    "- Assists in choosing appropriate modeling techniques.\n",
    "- Uncovers potential data quality issues early.\n",
    "\n",
    "`The following methods were employed to communicate our objective:`\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Conclusion and Future Work**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "\n",
    "\n",
    "##### Future Work\n",
    "\n",
    "To build upon this study, future work could focus on the following areas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#ten></a>\n",
    "## **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sections to Consider\n",
    "\n",
    "**Contributors**: Nozipho Sithembiso Ndebele & Thabisisle Xaba\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
